---
title: "Power analyses for microbiome studies"
output: html_notebook
bibliography: tutorial.bib
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

# Introduction

Power analyses are important for experimental study design so that the researcher has an idea of how many experiment subjects are needed to minimize Type II error. In microbiome studies power analyses can be difficult not just because the true effect size is unknown, but also because the composition of the microbiome in control and experimental groups (i.e. beta diversity) is generally unknown. However, if the study intends to use pairwise distances and PERMANOVA to measure diversity then we can use methods from the R package *micropower* [@Kelly2015] to perform power analyses as long as we have previously available datasets characteristic of one of the groups (most likely the control). The basic idea behind `micropower` is to simulate distance matrices given prior population parameters computed from previous studies, then simulate a range of effect sizes, rarefaction curves, etc. to estimate PERMANOVA power from the simulated distance matrices.

Here at the [Computational Biology Core at Brown](https://brown.edu/cis/data-science/compbiocore/) we have performed a number of power analyses for researchers wanting to do 16s and whole genome shotgun (which we will refer to as just shotgun from now on) metagenomics experiments. Following is a walkthrough of what such an analysis looks like as well as what you will need to consider for your own power analysis.

## Data {#data}

We will be using data from the Human Microbiome Project for this tutorial [@Methe2012]. The Human Microbiome Project has alleviated some of the difficulty in a lack of accessible and well characterized data by providing 16s and whole genome data on 300 healthy human subjects from the US in multiple tissue types.

In particular we will 1) use the [HMP1 Metadata Project Catalog](https://www.hmpdacc.org/hmp/catalog/) to find shotgun metagenomics gut samples, and then 2) match up the SRA IDs of the samples to the abundances table provided in the shotgun community profiling database [HMSCP](https://www.hmpdacc.org/hmp/HMSCP/). These abundances tables are what we will use to compute the necessary population parameters for the power analysis by transforming them into OTU tables.

(something about checking for effect sizes)

# Setting up

This entire analysis can be reproduced with this notebook and the files in the repository. If you don't already have the packages `micropower`, `devtools` and others installed then execute the commands below to install the packages.

```{r, install, eval=FALSE}
install.packages('devtools')
devtools::install_github("brendankelly/micropower")
install.packages('knitr')
install.packages('kableExtra')
install.packages('dplyr')
install.packages('vegan')
```

Afterwards we are going to load the packages we need. We will also set a random seed so as to make the results from this notebook fully reproducible.

```{r, setup}
library(micropower)
library(knitr)
library(kableExtra)
library(dplyr)
library(vegan)
set.seed(515087345)
```

We will assume that you have cloned the entire repository as well and thus have the same path to the data. If not then modify the variable `PATH` to point to the path of the data.

```{r, setup_path}
PATH='data'
```

# Analysis

The analysis consists of the following steps. First, we decide on what distance metric we will be using. For this tutorial we will be using unweighted Jaccard distance, but other metrics are possible as well. Then we find an appropriate dataset, then wrangle the data into the appropriate form. The necessary input for `micropower` is within-group mean and standard deviations. These can be computed using the chosen distance metric from OTU tables. Thus any dataset that is used should either provide within-group mean and standard deviations already, or provide data that can be transformed into an OTU table.

After within-group mean and standard deviations have been computed, 

## Finding, filtering and transforming the data

To start with, we have provided the project catalog from HMP as described in [Data](#data). You can download this yourself on their site by clicking on "HMP metagenomic samples" under *Browse datasets*, clicking on "View Project Catalog" at the bottom, and then clicking on the "Save to CSV" button. From here we filter for samples where the collection site is listed as `gastrointestinal_tract` (gut) and with gene counts greater than 0 (shotgun metagenomics as opposed to 16s) to get a list of SRA IDs. This results in 147 samples.

```{r,get_data}
# getting sample IDs
project_catalog<-read.csv(file.path(PATH,"project_catalog.csv"))
kable(head(project_catalog), caption="A few rows of values and column names in project catalog.") %>% kable_styling()

IDs <- select(project_catalog, Sequence.Read.Archive.ID, HMP.Isolation.Body.Site, Gene.Count) %>%
  filter(HMP.Isolation.Body.Site=='gastrointestinal_tract' & Gene.Count>0) %>%
  .$Sequence.Read.Archive.ID
length(IDs)
```

With the list of SRA IDs we can then download abundance tables associated with those SRA IDs. Here we have provided them in the Github repo for you, but below is the command you can use to download them using FTP. Note that if you plan to download a large amount of data from the HMP project that they provide a client [Aspera](https://www.hmpdacc.org/hmp/resources/download.php) to do so.

```{r,download,eval=FALSE}
lapply(IDs, function(x) download.file(url=paste0("ftp://public-ftp.ihmpdcc.org/HMSCP/",x,"/",x,"_abundance_table.tsv.bz2"),destfile=paste0(x,".tsv.bz2")))
```

The abundance tables from HMP need to be set up so that columns are by sample (SRA IDs), rows are by reference name, and entries are by the number of reads per sample per reference name i.e. like an OTU table. In addition, the information provided in each sample's abundance table is depth, breadth, and total reference bases. Depth is the number of times the reference was covered, breadth is the proportion of reference bases that were covered, and total reference bases is the length of the genome. In addition, the length of reads in sequencing was 100, and the breadth is given as a number out of 100. To get the number of reads covering a reference then we take the depth x total reference bases x breadth / (100 x 100). We then join the 147 samples together as columns.

```{r,make_table, warnings=FALSE}
tsvs<-lapply(IDs, function(x) read.csv(file.path(PATH,paste0(x,'.tsv')),sep='\t') %>% transmute(Reference.Name=Reference.Name,Nreads=Depth*Breadth*Total.reference.bases/10000))
otu_table<-Reduce(function(...) full_join(...,by="Reference.Name"), test)
colnames(test2) <- c("Reference.Name",as.character(IDs))
```

## Within-group Jaccard distances

Now that we have an OTU table we can compute distances. For our tutorial we will compute Jaccard distances, as they are built into `micropower`

also allows for Unifrac distances [@Lozupone2005] although you will need to provide an appropriate phylogenetic tree.

```{r,within_dist}
jaccard<-vegdist(test2[,2:148],method="jaccard",na.rm=TRUE)
m<-mean(jaccard[!is.na(jaccard)]) #0.8820892
s<-sd(jaccard[!is.na(jaccard)]) #0.1969525
```

## hashMean and simPower

```{r,hashing,include=FALSE}
unweight_hm<-hashMean(rare_levels=runif(1000,0,1),rep_per_level=10,otu_number=1000,sequence_depth=1)
means<-mclapply(unweight_hm, function(x) (mean(lowerTriDM(calcUJstudy(x)))),mc.cores=4)
pass_m<-means[which(means < 0.8821 & means>0.8820)] #0.23736 = depth of subsampling
unweight_hsd<-hashSD(0.23736,otu_number_range=c(10,100,1000,10000),sim_number=100)
sds<-mclapply(unweight_hsd, function(x) (sd(lowerTriDM(calcUJstudy(x)))), mc.cores=4) 
pass_s<-sds[which(sds<0.20 & sds>0.19)] #10 for number of OTUs
```

```{r, simPower}
sp<-simPower(group_size_vector=c(50,50), otu_number=10, rare_depth=0.23736,effect_range=seq(0,0.3,length.out=100))
uj<-mclapply(sp,function(x) calcUJstudy(x),mc.cores=4)
# check if mean & sd is approximately OK
mclapply(uj, mean, mc.cores=4)
mclapply(uj, sd, mc.cores=4)
```

```{r, bootPower}
bp10<-bootPower(uj, boot_number=100, subject_group_vector=c(10,10),alpha=0.05)
bp25<-bootPower(uj, boot_number=100, subject_group_vector=c(25,25),alpha=0.05)
bp50<-bootPower(uj, boot_number=100, subject_group_vector=c(50,50),alpha=0.05)
write.table(bp,file="boot.txt",row.names = FALSE)
```

```{r, plot}
ggplot2::qplot(data=subset(bp50,power>0.2 & power < 0.95),x=log10(power),y=log10(simulated_omega2)) + geom_smooth(method="lm") + ggtitle("Omega2 to Power with 50 Subjects/Group") + xlab("Omega2") + ylab("Power")

ggplot2::qplot(data=subset(bp50,power>0.2 & power < 0.95),x=log10(power),y=log10(simulated_omega2)) + geom_smooth(method="lm") + ggtitle("log10(Omega2) to log10(Power) with 50 Subjects/Group") + xlab("log10(Omega2)") + ylab("log10(Power)")

bp50_model <- subset(bp50, power < 0.95 & power > 0.2)
bp50_model <- data.frame(log_omega2=log10(bp50_model$simulated_omega2),log_power=log10(bp50_model$power))
bp50_model <- subset(bp50_model, log_omega2>-Inf)
bp50_model <- lm(log_omega2 ~ log_power, data=bp50_model)
10^predict(bp50_model, newdata=data.frame(log_power=log10(0.8))) # 80% power for omega2 of 0.009
10^predict(bp50_model, newdata=data.frame(log_power = log10(0.9))) # 90% power for omega2 of 0.01379
```

# References